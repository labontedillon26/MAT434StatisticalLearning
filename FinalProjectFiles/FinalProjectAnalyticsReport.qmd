---
title: Final Project Analytics Report
author: 
  - name: Dillon Labonte
    email: dillon.labonte@snhu.edu
    affiliations: 
      - name: Southern New Hampshire University
date: today
date-format: long
theme: lux
toc: true
code-fold: true
warning: false
message: false
---

```{r setup}
# including necessary libraries
library(tidyverse)
library(tidymodels)
library(patchwork)
library(kableExtra)
library(ggmosaic)
library(rpart.plot)

# styling
options(kable_styling_bootstrap_options = c("hover", "striped"))
theme_set(theme_bw(base_size = 14))

# read in data
data <- read_csv("FourClassDS.csv")

data <- data |>
  rename(num = ...1)

data <- data |>
  mutate(id = as.factor(id))

# split data into training, test, and validation sets
set.seed(1234567890)
data_splits <- initial_split(data, prop = 0.75)

train <- training(data_splits)
test <- testing(data_splits)

unregister <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

train_folds <- vfold_cv(train, v = 5)

```

## Statement of Purpose

The purpose of this report is to analyze and build predictive models for a LiDAR data frame. This data was collected from RPLiDAR A1 unit by Slamtech and features 360 distance measurements as well as an environment id column. The environment id corresponds to the following: room: 0, corridor: 1, doorway: 2, hall: 3. By building a predictive classification model, robots utilizing LiDAR technology will be able to more accurately classify the environment they are in. This is crucial for autonomous robot navigation.  

## Executive Summary

This report explores the use of machine learning techniques to identify the environment type (room, corridor, doorway, hall) based off of scans from the RPLiDAR A1 unit by Slamtech. Each observation in the dataset consists of $360$ measurements from a LiDAR scan, as well as an environment id column to specify the environment the LiDAR unit was placed in. Accurate classification of environments is important for robot sensing and navigation, becoming more and more prevalent with the increasing popularity of self driving cars.

An exploratory data analysis was performed on the dataset to get a sense of what we are working with. Dimension reduction steps (principal component analysis) were necessary due to the small size of the dataset. Three model classes were built and assessed: decision tree, random forest, and boosted tree models. Ultimately, the random forest model performed the best, with an accuracy of $94 \%$ on the training data, and $93 \%$ on the test set.

Although the results of model construction indicate decent performance, the model construction section was limited by the small number of observations and the need for principal component analysis. The need for principal component analysis would be eliminated in the case of a large data set, and would likely improve the overall performance of every model class.

This project highlights the importance of machine learning in an engineering context. Similar approaches can be taken to a number of other engineering problems and will prove to be an important skill.

## Introduction

Light detection and ranging (LiDAR) technology provides precise and high resolution information about complex 3D environments. LiDAR technology works by emitting rapid pulses of, often infrared, light and measuring the return time back to the source of the pulse after bouncing off an object. Since the speed of light is precisely known, the distance to the object can be calculated with high precision. After computing these distances for an entire area, a precise 3D map of the environment can be generated.

The LiDAR technology works well even in low light environments as well as through thin vegetation, unlike cameras. LiDAR can be used to created detailed topographical maps and identify archaelogic structures through forest canopies and other vegetation. Autonomous vehicles rely heavily on LiDAR technology to detect obstacles, map the surrounding environment, and navigate safely. Due to the speed of light being incrompehensibly fast, LiDAR technology scans and maps environments faster than it could ever be completed manually.

LiDAR technology encompasses the components that are needed to measure and map environments, but is unable to 'think' on its own or identify the type of environment it is in. Thus, it is necessary to have a software system as well as some sort of machine learning model to assist the LiDAR technology in the desired operations. This report uses a LiDAR data frame from Slamtec for their RPLiDAR A1 unit. The dataset consists of `r nrow(data)` observations, with each specifying a scan for a particular environment. By using machine learning techniques and predictive classification models, we are able to predict with decent accuracy what environment type the LiDAR unit is in based on what it is seeing. This sort of analysis is crucial for the advancement of LiDAR technology and robot sensing, and will find practical applications in the world of autonomous vehicles and beyond.

## Exploratory Data Analysis

In this section we explore the LiDAR dataset to get an idea of what we're working with.

The data set has `r nrow(data)` observations, split up into `r nrow(train)` training observations and `r nrow(test)` testing observations. 

Let's look at the first $5$ rows of the dataset.

```{r}

train |>
  head(5) |>
  kable() |>
  kable_styling()

```

Each observation has a number attached to it, $360$ LiDAR scan values (in millimeters) and in the last column, an id to specify which environment type the LiDAR module was placed into.

Next let's look at the spread of our response variable, the environment id.

```{r}

train |>
  count(id) |>
  rename(ID = id) |>
  kable() |>
  kable_styling()

```

```{r}

train |>
  ggplot() +
  geom_bar(aes(x = id), fill = 'steelblue') +
  labs(x = "Environment ID", y = "Count")

```

The above table and plot show a mostly uniform distribution of the response variable which is quite advantageous for a classification problem, especially with such a small dataset.

The method by which the LiDAR device records values is by rotating on an axis and measuring the distance to the nearest object using pulses of [usually] infrared light. Thus, a plot of the scan values versus the angle will nicely fit into a polar plot, allowing us to see what the robot "sees".

Let's look at two examples of each environment type.

```{r}

dataviz_train <- train |>
  pivot_longer(
    cols = -c(num, id),
    names_to = "Degree",
    values_to = "Measure"
  )

```

```{r}

room1 <- dataviz_train |>
  filter(num == 97) |>
  ggplot() +
  geom_point(aes(x = Degree, y = Measure)) +
  coord_polar(theta = "x") +
  theme(
    axis.title.x = element_blank(),  # Removes x-axis label
    axis.title.y = element_blank(),  # Removes y-axis label
    axis.text.x = element_blank(),   # Removes x-axis text
    axis.text.y = element_blank(),   # Removes y-axis text
    axis.ticks = element_blank()     # Removes axis ticks
  ) +
  labs(title = "Example Room 1")

room2 <- dataviz_train |>
  filter(num == 47) |>
  ggplot() +
  geom_point(aes(x = Degree, y = Measure)) +
  coord_polar(theta = "x") +
  theme(
    axis.title.x = element_blank(),  # Removes x-axis label
    axis.title.y = element_blank(),  # Removes y-axis label
    axis.text.x = element_blank(),   # Removes x-axis text
    axis.text.y = element_blank(),   # Removes y-axis text
    axis.ticks = element_blank()     # Removes axis ticks
  ) +
  labs(title = "Example Room 2")

corridor1 <- dataviz_train |>
  filter(num == 198) |>
  ggplot() +
  geom_point(aes(x = Degree, y = Measure)) +
  coord_polar(theta = "x") +
  theme(
    axis.title.x = element_blank(),  # Removes x-axis label
    axis.title.y = element_blank(),  # Removes y-axis label
    axis.text.x = element_blank(),   # Removes x-axis text
    axis.text.y = element_blank(),   # Removes y-axis text
    axis.ticks = element_blank()     # Removes axis ticks
  ) +
  labs(title = "Example Corridor 1")

corridor2 <- dataviz_train |>
  filter(num == 206) |>
  ggplot() +
  geom_point(aes(x = Degree, y = Measure)) +
  coord_polar(theta = "x") +
  theme(
    axis.title.x = element_blank(),  # Removes x-axis label
    axis.title.y = element_blank(),  # Removes y-axis label
    axis.text.x = element_blank(),   # Removes x-axis text
    axis.text.y = element_blank(),   # Removes y-axis text
    axis.ticks = element_blank()     # Removes axis ticks
  ) +
  labs(title = "Example Corridor 2")

doorway1 <- dataviz_train |>
  filter(num == 300) |>
  ggplot() +
  geom_point(aes(x = Degree, y = Measure)) +
  coord_polar(theta = "x") +
  theme(
    axis.title.x = element_blank(),  # Removes x-axis label
    axis.title.y = element_blank(),  # Removes y-axis label
    axis.text.x = element_blank(),   # Removes x-axis text
    axis.text.y = element_blank(),   # Removes y-axis text
    axis.ticks = element_blank()     # Removes axis ticks
  ) +
  labs(title = "Example Doorway 1")

doorway2 <- dataviz_train |>
  filter(num == 279) |>
  ggplot() +
  geom_point(aes(x = Degree, y = Measure)) +
  coord_polar(theta = "x") +
  theme(
    axis.title.x = element_blank(),  # Removes x-axis label
    axis.title.y = element_blank(),  # Removes y-axis label
    axis.text.x = element_blank(),   # Removes x-axis text
    axis.text.y = element_blank(),   # Removes y-axis text
    axis.ticks = element_blank()     # Removes axis ticks
  ) +
  labs(title = "Example Doorway 2")

hall1 <- dataviz_train |>
  filter(num == 25) |>
  ggplot() +
  geom_point(aes(x = Degree, y = Measure)) +
  coord_polar(theta = "x") +
  theme(
    axis.title.x = element_blank(),  # Removes x-axis label
    axis.title.y = element_blank(),  # Removes y-axis label
    axis.text.x = element_blank(),   # Removes x-axis text
    axis.text.y = element_blank(),   # Removes y-axis text
    axis.ticks = element_blank()     # Removes axis ticks
  ) +
  labs(title = "Example Hall 1")

hall2 <- dataviz_train |>
  filter(num == 43) |>
  ggplot() +
  geom_point(aes(x = Degree, y = Measure)) +
  coord_polar(theta = "x") +
  theme(
    axis.title.x = element_blank(),  # Removes x-axis label
    axis.title.y = element_blank(),  # Removes y-axis label
    axis.text.x = element_blank(),   # Removes x-axis text
    axis.text.y = element_blank(),   # Removes y-axis text
    axis.ticks = element_blank()     # Removes axis ticks
  ) +
  labs(title = "Example Hall 2")

```

```{r}

room1 + room2

```

```{r}

corridor1 + corridor2

```

```{r}

doorway1 + doorway2

```

```{r}

hall1 + hall2

```

These plots are quite interesting. The two room examples look very neat and are easy to identify as rooms to the human eye. Hall example 2 also shows a decent recreation of the environment. However, the remaining examples do not show much information about the environment and look very sporadic. This may have a bad effect on our model construction and could limit the performance of our models. Improving the LiDAR technology would likely improve the abilities of our predictive models.

This exploratory data analysis showed us what our dataset consisted of, `r nrow(data)` observations each with $360$ scan values and an environment id. We saw that the environment id response variable was mostly uniformly distributed which will be advantageous for our model construction section. Lastly, we looked at some polar plots of the LiDAR scans to get a sense of what the unit "sees."

## Model Construction

In this section we explore various classification model classes and their performance on predicting the environment. It is important to note that since there are lots of columns in the dataset compared to number of rows, it is necessary to use dimension reduction steps such as principal component analysis. This may limit the predictive ability of our models but it is a necessary step.

### Decision Tree Model

The first model we build is a simple decision tree which asks yes or no questions and will provide a baseline before moving into more complex model classes.

#### Without Cross Validation

We begin by looking at a simple decision tree model without any cross validation or hyperparameter tuning.

```{r}

dt_spec <- decision_tree() |>
  set_mode("classification")

dt_rec <- recipe(id ~ ., data = train) |>
  step_rm(num) |>
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = 5)

dt_wf <- workflow() |>
  add_model(dt_spec) |>
  add_recipe(dt_rec)

dt_fit <- dt_wf |>
  fit(train)

```

```{r}

dt_fit |>
  augment(train) |>
  accuracy(id, .pred_class) |>
  select(!.estimator) |>
  rename(Metric = .metric) |>
  rename(Estimate = .estimate) |>
  kable() |>
  kable_styling()

```

This baseline model actually does quite well, however this is not necessarily a good indicator of how well the model is doing since it is prone to overfitting in the training data.

#### With Cross Validation

To get a better idea of how our model is doing, we use cross validation which will limit the overfitting effect.

```{r}

dt_cv_spec <- decision_tree() |>
  set_mode("classification")

dt_cv_rec <- recipe(id ~ ., data = train) |>
  step_rm(num) |>
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = 5)

dt_cv_wf <- workflow() |>
  add_model(dt_cv_spec) |>
  add_recipe(dt_cv_rec)

```

```{r}
#| eval: false

dt_cv_results <- dt_cv_wf |>
  fit_resamples(
    resamples = train_folds,
    metrics = metric_set(accuracy)
  )

```

```{r}
#save(dt_cv_results, dt_cv_wf, file = "dt_cv_results.RData")
load("dt_cv_results.RData")

dt_cv_results |>
  collect_metrics() |>
  select(!c(.config, .estimator, n)) |>
  rename(Metric = .metric) |>
  rename(Mean = mean) |>
  rename(Standard_Error = std_err) |>
  kable() |>
  kable_styling()

```

The cross validation results show an accuracy of $82 \%$, which is a more accurate assessment of our model's performance.

#### Hyperparameter Tuning

To improve the accuracy of the simple decision tree model, hyperparameter tuning is used to find the optimal hyperparameter settings for our model. The parameters that are tuned for the tree are `cost_complexity`, `tree_depth`, and `min_n`, and `num_comp` was tuned for principal component analysis.

```{r}

dt_tune_spec <- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) |>
  set_mode("classification")

dt_tune_rec <- recipe(id ~ ., data = train) |>
  step_rm(num) |>
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = tune())

dt_tune_wf <- workflow() |>
  add_model(dt_tune_spec) |>
  add_recipe(dt_tune_rec)

dt_param_grid <- crossing(
  num_comp = c(1:10),
  cost_complexity = 10^seq(-5, 0, length.out = 6),
  tree_depth = c(1:10),
  min_n = c(1:5),
) 

```

```{r}
#| eval: false

n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cl)

tictoc::tic()

dt_tune_results <- dt_tune_wf %>%
  tune_grid(
    resamples = train_folds,
    metrics = metric_set(accuracy),
    grid = dt_param_grid,
    control = control_grid(parallel_over = "everything")
  )

tictoc::toc()

doParallel::stopImplicitCluster()
unregister()

```

```{r dt-tune-results}
#save(dt_tune_results, dt_tune_wf, file = "dt_tune_results.RData")
load("dt_tune_results.RData")

dt_tune_results |>
  show_best(n = 10, metric = "accuracy") |>
  select(!.config) |>
  select(!.estimator) |>
  select(!n) |>
  rename(Cost_Complexity = cost_complexity) |>
  rename(Tree_Depth = tree_depth) |>
  rename(Min_n = min_n) |>
  rename(Number_of_Components = num_comp) |>
  rename(Metric = .metric) |>
  rename(Mean = mean) |>
  rename(Standard_Error = std_err) |>
  kable() |>
  kable_styling()

```

The results of this hyperparameter tuning process show significant improvement from the cross validation model, increasing from $82 \%$ accuracy to about $92 \%$.

### Visualizing the Decision Tree

The following plot shows a visual of the decision tree model. 

```{r}

best_dt <- dt_tune_results |>
  select_best(metric = "accuracy")

dt_wf_final <- dt_tune_wf |>
  finalize_workflow(best_dt)

dt_fit <- dt_wf_final |>
  fit(train)

dt_fit |>
  extract_fit_engine() |>
  rpart.plot()

```

Since principal component analysis was used to reduce the number of effective columns in the dataset, the decision tree becomes harder to interpret. The principal component values do not have an obvious connection to specific scan values so it is hard to spot trends.

### Random Forest Model

In this section we start to explore more complex model classes; ensembles. The first we look at is a random forest model which utilizes a large number of decision trees in parallel to improve model performance.

#### With Cross Validation

We begin by jumping straight into a cross validation model to have a good understanding of our model performance.

```{r}

rf_cv_spec <- rand_forest() |>
  set_mode("classification")

rf_cv_rec <- recipe(id ~ ., data = train) |>
  step_rm(num) |>
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = 5)

rf_cv_wf <- workflow() |>
  add_model(rf_cv_spec) |>
  add_recipe(rf_cv_rec)

```

```{r}
#| eval: false

rf_cv_results <- rf_cv_wf |>
  fit_resamples(
    resamples = train_folds,
    metrics = metric_set(accuracy)
  )

```

```{r}
#save(rf_cv_results, rf_cv_wf, file = "rf_cv_results.RData")
load("rf_cv_results.RData")

rf_cv_results |>
  collect_metrics() |>
  select(!c(.config, .estimator, n)) |>
  rename(Metric = .metric) |>
  rename(Mean = mean) |>
  rename(Standard_Error = std_err) |>
  kable() |>
  kable_styling()

```

This cross validation based random forest model has an accuracy of $90 \%$, much better than the $82 \%$ accuracy that the decision tree model had.

#### Hyperparameter Tuning

The accuracy of the random forest model can be further improved using hyperparameter tuning. The parameters that are tuned for the model are `trees` and `min_n`, and `num_comp` is tuned for principal component analysis.

```{r}

rf_tune_spec <- rand_forest(trees = tune(), min_n = tune()) |>
  set_mode("classification")

rf_tune_rec <- recipe(id ~ ., data = train) |>
  step_rm(num) |>
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = tune())

rf_tune_wf <- workflow() |>
  add_model(rf_tune_spec) |>
  add_recipe(rf_tune_rec)

rf_param_grid <- crossing(
  num_comp = c(1:10),
  trees = seq(1, 2000, by = 199),
  min_n = c(1:5),
) 

```

```{r}
#| eval: false

n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cl)

tictoc::tic()

rf_tune_results <- rf_tune_wf %>%
  tune_grid(
    resamples = train_folds,
    metrics = metric_set(accuracy),
    grid = rf_param_grid,
    control = control_grid(parallel_over = "everything")
  )

tictoc::toc()

doParallel::stopImplicitCluster()
unregister()

```


```{r rf-tune-results}
#save(rf_tune_results, rf_tune_wf, file = "rf_tune_results.RData")
load("rf_tune_results.RData")

rf_tune_results |>
  show_best(n = 10, metric = "accuracy") |>
  select(!c(.config, .estimator, n)) |>
  rename(Trees = trees) |>
  rename(Min_n = min_n) |>
  rename(Number_of_Components = num_comp) |>
  rename(Metric = .metric) |>
  rename(Mean = mean) |>
  rename(Standard_Error = std_err) |>
  kable() |>
  kable_styling()

```

With hyperparameter tuning, the accuracy of the random forest model increased to $94 \%$ which is pretty good and is the best we have seen so far. It's also important to note that the model was assessed on the training data. An assessment of the model's performance on a test set is in the `Model Assessment` section.

### Boosted Tree Model

The last model class we explore is a boosted tree model. This model class is also built upon decision trees, but unlike the random forest model, the decision trees are in series one after the other instead of in parallel.

#### With Cross Validation

Again, we jump into a model built using cross validation to get a more accurate understanding of our baseline model performance.

```{r}

bt_cv_spec <- boost_tree() |>
  set_mode("classification")

bt_cv_rec <- recipe(id ~ ., data = train) |>
  step_rm(num) |>
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = 5)

bt_cv_wf <- workflow() |>
  add_model(bt_cv_spec) |>
  add_recipe(bt_cv_rec)

```

```{r}
#| eval: false

bt_cv_results <- bt_cv_wf |>
  fit_resamples(
    resamples = train_folds,
    metrics = metric_set(accuracy)
  )

```

```{r}
#save(bt_cv_results, bt_cv_wf, file = "bt_cv_results.RData")
load("bt_cv_results.RData")

bt_cv_results |>
  collect_metrics() |>
  select(!c(.config, .estimator, n)) |>
  rename(Metric = .metric) |>
  rename(Mean = mean) |>
  rename(Standard_Error = std_err) |>
  kable() |>
  kable_styling()

```

The cross validation based boosted tree model has an accuracy of nearly $90 \%$. This is substantially better than the decision tree model accuracy of $82 \%$, and marginally worse than the random forest model accuracy of $90 \%$. 

#### Hyperparameter Tuning

To improve the accuracy of the boosted tree model, we again use hyperparameter tuning. The parameters tuned for the model are `trees`, `min_n`, `tree_depth`, and `learn_rate`, and `num_comp` is tuned for principal component analysis.

```{r}

bt_tune_spec <- boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune()) |>
  set_mode("classification")

bt_tune_rec <- recipe(id ~ ., data = train) |>
  step_rm(num) |>
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = tune())

bt_tune_wf <- workflow() |>
  add_model(bt_tune_spec) |>
  add_recipe(bt_tune_rec)

bt_param_grid <- crossing(
  num_comp = c(1:7),
  trees = seq(1, 11, by = 2),
  min_n = c(1:2),
  tree_depth = c(1:5),
  learn_rate = 10^(seq(-4, 0, by = 1))
) 

```

```{r}
#| eval: false

n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cl)

tictoc::tic()

bt_tune_results <- bt_tune_wf %>%
  tune_grid(
    resamples = train_folds,
    metrics = metric_set(accuracy),
    grid = bt_param_grid,
    control = control_grid(parallel_over = "everything", verbose = TRUE)
  )

tictoc::toc()

doParallel::stopImplicitCluster()
unregister()

```

```{r bt-tune-results}
#save(bt_tune_results, bt_tune_wf, file = "bt_tune_results.RData")
load("bt_tune_results.RData")

bt_tune_results |>
  show_best(n = 10, metric = "accuracy") |>
  select(!c(.config, .estimator, n)) |>
  rename(Trees = trees) |>
  rename(Min_n = min_n) |>
  rename(Tree_Depth = tree_depth) |>
  rename(Learn_Rate = learn_rate) |>
  rename(Number_of_Components = num_comp) |>
  rename(Metric = .metric) |>
  rename(Mean = mean) |>
  rename(Standard_Error = std_err) |>
  kable() |>
  kable_styling()

```

From this hyperparameter tuning process, we see that the boosted tree model accuracy has increased to about $92 \%$. This is a little better than the cross validation model, however it is still slightly worse than the random forest model.

### Model Construction Recap

In the model construction section, we built decision tree, random forest, and boosted tree models to predict our LiDAR environment type. The decision tree model modestly performed the worst ($92 \%$ accuracy) but provided us with more interpretive ability. The random forest and boosted tree models improved upon this ($94 \%$ and $92 \%$ accuracy, respectively), but lost the interpretive ability.

## Model Assessment

In the previous section, we explored various model classes and recorded their performance metrics. Let's analyze our best model class a little deeper.

### Best Model

The best model class we explored was the random forest model with an accuracy mean of about $94 \%$ from cross validation ($5$ folds). Let's fit this model to a couple of data sets and see how our model performs.

The first data set we fit to is the training data. We expect the model to do well here, since it knows the most about this data set.

```{r}

best_rf <- rf_tune_results |>
  select_best(metric = "accuracy")

rf_wf_final <- rf_tune_wf |>
  finalize_workflow(best_rf)

rf_fit <- rf_wf_final |>
  fit(train)

rf_fit |>
  augment(train) |>
  accuracy(id, .pred_class) |>
  select(!.estimator) |>
  rename(Metric = .metric) |>
  rename(Estimate = .estimate) |>
  kable() |>
  kable_styling()

```

Here, we see an accuracy of $100 \%$! This is obviously quite optimistic and should not inform our opinions about our model's performance.

Next we look at the holdout `test` set so that we can have a better understanding of our model's performance on new data.

```{r}

rf_fit |>
  augment(test) |>
  accuracy(id, .pred_class) |>
  select(!.estimator) |>
  rename(Metric = .metric) |>
  rename(Estimate = .estimate) |>
  kable() |>
  kable_styling()

```

Here, we see an accuracy of $93 \%$, which is worse than the accuracy on the training set as expected. However, $93 \%$ is still quite good and indicates that our model is performing well.

Our best model performs fairly well on the test set, however due to this being such a small data set, the probability that the model got 'lucky' is higher. This is especially prevalent in this dataset, with only $4$ response classes. To get a better idea of our model's performance, it would need to be tested on more data which is not currently available.

Another factor that is limiting our model's capabilities is the necessity for dimension reduction using principal component analysis. This problem would be resolved with a much larger dataset and would likely improve the performance of the model and allow for more interpretation in the simple decision tree case. 

## Conclusion

This analytics report provided a detailed approach to predictive modeling for LiDAR dataframes. LiDAR, a technology that uses light pulses to map complex 3D environments, is an essential technology for autonomous navigation in commercial and industrial applications. By using a predictive machine learning model to predict the environment type of a LiDAR module, autonomous navigation and obstacle detection can be improved. This is crucial for the effectiveness and safety of self driving cars as they become increasingly popular.

The exploratory data analysis revealed that our dataset is quite small and dimension reduction techniques such as principal component analysis would need to be applied. It was also found that the response variable, the environment id, was relatively uniformly distributed which is advantageous for a classification problem. To finish off the exploratory data analysis section, we looked at two examples of each environment type, displayed using polar coordinates, to get a sense of what the LiDAR unit was 'seeing'. 

Model construction started with a simple decision tree model which allowed for some interpretation but was mainly used as a baseline model for assessment of more complex model classes. After hyperparameter tuning, all three model classes built (decision tree, random forest, boosted tree) had an accuracy of greater than $90 \%$!. The random forest model was slightly better than the other two, with an accuracy of $94 \%$ on the training set, and $93 \%$ on the test set. This would likely be further improved if a much larger dataset was used, which would eliminate the need for dimension reduction steps.

This project has introduced LiDAR technology and applied tools developed in this course to a real world problem that is relevant to the field of engineering. This project has invoked thought about robot sensing and navigation and raises questions of other opportunities in the field of engineering, specifically electrical, where machine learning techniques can be applied to further enhance our technologies. Data science is an important skill to have in industry and will provide interesting opportunities to those that study it.

## References

[1]OpenAI, “ChatGPT,” ChatGPT, Apr. 16, 2025. https://chatgpt.com/

[2]T. Huang, “思岚科技-机器人自主定位导航方案 (激光雷达,SLAM,机器人底盘) 提供商,” Slamtec.com, 2016. https://www.slamtec.com/ (accessed Apr. 21, 2025).

[3]Tareq Alhmiedat, “LidarDataFrames,” Kaggle.com, 2022. https://www.kaggle.com/datasets/tareqalhmiedat/lidardataframes (accessed Apr. 21, 2025).
